{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fac6ee-e1d4-4c97-86b7-84114cc6cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp as kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "import os\n",
    "from kfp.components import InputPath, OutputPath, create_component_from_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85f9ff83-ffd6-4949-a2c9-bba842bb7f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(\n",
    "    training_data_path: InputPath('CSV'),  # Also supports LibSVM\n",
    "    model_path: OutputPath('XGBoostModel'),\n",
    "    model_config_path: OutputPath('XGBoostModelConfig'),\n",
    "    training_log_path: OutputPath(),\n",
    "    starting_model_path: InputPath('XGBoostModel') = None,\n",
    "    \n",
    "\n",
    "    label_column: int = 0,\n",
    "    num_iterations: int = 10,\n",
    "    booster_params: dict = None,\n",
    "\n",
    "    # Booster parameters\n",
    "    objective: str = 'reg:squarederror',\n",
    "    booster: str = 'gbtree',\n",
    "    learning_rate: float = 0.3,\n",
    "    min_split_loss: float = 0,\n",
    "    max_depth: int = 6,\n",
    "):\n",
    "    '''Train an XGBoost model.\n",
    "    Args:\n",
    "        training_data_path: Path for the training data in CSV format.\n",
    "        model_path: Output path for the trained model in binary XGBoost format.\n",
    "        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.\n",
    "        starting_model_path: Path for the existing trained model to start from.\n",
    "        label_column: Column containing the label data.\n",
    "        num_boost_rounds: Number of boosting iterations.\n",
    "        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "        objective: The learning task and the corresponding learning objective.\n",
    "            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n",
    "            The most common values are:\n",
    "            \"reg:squarederror\" - Regression with squared loss (default).\n",
    "            \"reg:logistic\" - Logistic regression.\n",
    "            \"binary:logistic\" - Logistic regression for binary classification, output probability.\n",
    "            \"binary:logitraw\" - Logistic regression for binary classification, output score before logistic transformation\n",
    "            \"rank:pairwise\" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n",
    "            \"rank:ndcg\" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n",
    "\n",
    "    '''\n",
    "    import pandas\n",
    "    import xgboost\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from tensorboardX import SummaryWriter\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pyarrow\n",
    "    \n",
    "    \n",
    "    ### embedded function to allow tensorboard to monitor the training ###\n",
    "    def TensorBoardCallback():\n",
    "        writer = SummaryWriter(training_log_path)\n",
    "\n",
    "        def callback(env):\n",
    "            for k, v in env.evaluation_result_list:\n",
    "                print(k,v)\n",
    "                writer.add_scalar(k, v, env.iteration)\n",
    "\n",
    "        return callback\n",
    "    \n",
    "    ### load data ###\n",
    "    \n",
    "    df = pandas.read_parquet(\n",
    "        training_data_path,\n",
    "    )\n",
    "    \n",
    "    ### autoclean data to allow only copatible types in features\n",
    "    numerics = ['int','float']\n",
    "    df = df.select_dtypes(include=numerics)\n",
    "    \n",
    "    ### split data ###\n",
    "\n",
    "    data=df.drop(columns=[df.columns[label_column]])\n",
    "    label=df[df.columns[label_column]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=100)\n",
    "    \n",
    "    dtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgboost.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    ### model HP ###\n",
    "\n",
    "    booster_params = booster_params or {}\n",
    "    booster_params.setdefault('objective', objective)\n",
    "    booster_params.setdefault('booster', booster)\n",
    "    booster_params.setdefault('learning_rate', learning_rate)\n",
    "    booster_params.setdefault('min_split_loss', min_split_loss)\n",
    "    booster_params.setdefault('max_depth', max_depth)\n",
    "    \n",
    "    ### Not from scratch training management ###\n",
    "\n",
    "    starting_model = None\n",
    "    if starting_model_path:\n",
    "        starting_model = xgboost.Booster(model_file=starting_model_path)\n",
    "\n",
    "    ### Model fit to data ###\n",
    "\n",
    "    model = xgboost.train(\n",
    "        params=booster_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=num_iterations,\n",
    "        xgb_model=starting_model,\n",
    "        evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "       callbacks=[TensorBoardCallback()]\n",
    "    )\n",
    "        \n",
    "    ### Save the model as an artifact ###\n",
    "    model.save_model(model_path)\n",
    "\n",
    "    model_config_str = model.save_config()\n",
    "    with open(model_config_path, 'w') as model_config_file:\n",
    "        model_config_file.write(model_config_str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cb8655-ff96-4f71-846b-5d02a2a58c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Xgboost train(training_data: 'CSV', starting_model: 'XGBoostModel' = None, label_column: int = '0', num_iterations: int = '10', booster_params: dict = None, objective: str = 'reg:squarederror', booster: str = 'gbtree', learning_rate: float = '0.3', min_split_loss: float = '0', max_depth: int = '6')>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_component_from_func(\n",
    "    xgboost_train,\n",
    "    output_component_file='xgb_train_dbg.yaml',\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=[\n",
    "        'xgboost==1.1.1',\n",
    "        'pandas==1.0.5',\n",
    "        'tensorboardX==2.5.1',\n",
    "        'scikit-learn==1.0',\n",
    "        'pyarrow==10.0.1'\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a352bffd-e7ef-4764-baeb-474d546040d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
